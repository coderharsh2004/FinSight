{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm5LIdNUhLV8",
        "outputId": "1b54d418-4b1a-4c58-e641-67c08d5ddde9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping newspaper3k as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: lxml 5.3.1\n",
            "Uninstalling lxml-5.3.1:\n",
            "  Successfully uninstalled lxml-5.3.1\n",
            "Collecting lxml\n",
            "  Downloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml, lxml_html_clean\n",
            "Successfully installed lxml-5.3.1 lxml_html_clean-0.4.1\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.17.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=c126774b0cc15b2c198d177f6b56d76b6a666442cd487965ab14b39edc0228c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=8220920f91a309f50bdc063f93a5d2f9dfd8c4ca6f44a1dbfee17397f49b5155\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=52ac7d7567db4011f5fa488aefb01fe2d5a6dbb38e7684e52ff3118c32fbbb26\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=0faf52db15056f100285ea414e5aa5626cb534e359375e714a82d0f1362ea183\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y newspaper3k lxml\n",
        "!pip install --upgrade lxml lxml_html_clean\n",
        "!pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rHUy2bnhlI-",
        "outputId": "2e3457cf-7645-4cfb-c9ae-78a89d6121eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uvPGvSFhqr5",
        "outputId": "7d9ca908-e911-4717-a486-60534b21b2b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GoogleNews\n",
            "  Downloading GoogleNews-1.6.15-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from GoogleNews) (4.13.3)\n",
            "Collecting dateparser (from GoogleNews)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from GoogleNews) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->GoogleNews) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->GoogleNews) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->GoogleNews) (2025.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser->GoogleNews) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->GoogleNews) (5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->GoogleNews) (1.17.0)\n",
            "Downloading GoogleNews-1.6.15-py3-none-any.whl (8.8 kB)\n",
            "Downloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/295.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dateparser, GoogleNews\n",
            "Successfully installed GoogleNews-1.6.15 dateparser-1.2.1\n",
            "Collecting yahoo_fin\n",
            "  Downloading yahoo_fin-0.8.9.1-py3-none-any.whl.metadata (699 bytes)\n",
            "Collecting requests-html (from yahoo_fin)\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.11/dist-packages (from yahoo_fin) (6.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from yahoo_fin) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from yahoo_fin) (2.2.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser->yahoo_fin) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->yahoo_fin) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->yahoo_fin) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->yahoo_fin) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->yahoo_fin) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->yahoo_fin) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->yahoo_fin) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->yahoo_fin) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->yahoo_fin) (2025.1.31)\n",
            "Collecting pyquery (from requests-html->yahoo_fin)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html->yahoo_fin)\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html->yahoo_fin)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting bs4 (from requests-html->yahoo_fin)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting w3lib (from requests-html->yahoo_fin)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html->yahoo_fin)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html->yahoo_fin)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html->yahoo_fin)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.67.1)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->yahoo_fin)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html->yahoo_fin)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->yahoo_fin) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->requests-html->yahoo_fin) (4.13.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html->yahoo_fin) (5.3.1)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html->yahoo_fin) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->requests-html->yahoo_fin) (2.6)\n",
            "Downloading yahoo_fin-0.8.9.1-py3-none-any.whl (10 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyquery, pyee, fake-useragent, pyppeteer, bs4, requests-html, yahoo_fin\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 0.8.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 fake-useragent-2.0.3 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4 yahoo_fin-0.8.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install GoogleNews\n",
        "!pip install yahoo_fin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --index-url=https://blpapi.bloomberg.com/repository/releases/python/simple/ blpapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbsZzgwqj_L_",
        "outputId": "edc9e574-8f0f-43c5-f57c-f3936ef275c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://blpapi.bloomberg.com/repository/releases/python/simple/\n",
            "Collecting blpapi\n",
            "  Downloading https://blpapi.bloomberg.com/repository/releases/python/blpapi-3.25.1-py3-none-manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blpapi\n",
            "Successfully installed blpapi-3.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import csv\n",
        "import nltk\n",
        "import spacy\n",
        "from newspaper import Article\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from GoogleNews import GoogleNews\n",
        "from yahoo_fin import news\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize NLP tools\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Stock Information for Amazon\n",
        "stock_name = \"Amazon\"\n",
        "stock_ticker = \"AMZN\"\n",
        "keywords = [\"Amazon\", \"AMZN\", \"Jeff Bezos\", \"AWS\", \"E-commerce\", \"Amazon Prime\", \"Cloud Computing\"]\n",
        "\n",
        "# API Keys\n",
        "ALPHA_VANTAGE_KEY = \"8HOXVJD6FABJBNUQ\"\n",
        "NEWSAPI_KEY = \"a2bcdb25ffb4404ea911aa363e05b48c\"\n",
        "STOCK_NEWS_API_KEY = \"xvhgrwydlxpcg5pr1isujdpysapupc0s1sva8w9f\"\n",
        "\n",
        "# Function to check relevance\n",
        "def is_relevant(text):\n",
        "    return any(keyword in text for keyword in keywords)\n",
        "\n",
        "# Function to classify sentiment\n",
        "def classify_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    if score > 0.1:\n",
        "        return \"Positive\"\n",
        "    elif score < -0.1:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Fetch news URLs from APIs\n",
        "def get_newsapi_links(num_articles=500):\n",
        "    url = f\"https://newsapi.org/v2/everything?q={stock_name}&sortBy=publishedAt&language=en&apiKey={NEWSAPI_KEY}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        return [article[\"url\"] for article in data.get(\"articles\", [])[:num_articles]]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching NewsAPI: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_google_news_links(num_articles=500):\n",
        "    googlenews = GoogleNews(lang=\"en\", period=\"7d\")\n",
        "    googlenews.search(stock_name)\n",
        "    results = googlenews.result()\n",
        "    return [article[\"link\"] for article in results[:num_articles]]\n",
        "\n",
        "def get_yahoo_finance_links(num_articles=500):\n",
        "    try:\n",
        "        articles = news.get_yf_rss(stock_ticker)\n",
        "        return [article[\"link\"] for article in articles[:num_articles]]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Yahoo Finance: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_alpha_vantage_links(num_articles=500):\n",
        "    url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={stock_ticker}&apikey={ALPHA_VANTAGE_KEY}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        return [article[\"url\"] for article in data.get(\"feed\", [])[:num_articles]]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Alpha Vantage: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_stocknewsapi_links(num_articles=5000, batch_size=100):\n",
        "    collected_urls = set()\n",
        "    page = 1\n",
        "    while len(collected_urls) < num_articles:\n",
        "        url = f\"https://stocknewsapi.com/api/v1?tickers={stock_ticker}&items={batch_size}&page={page}&token={STOCK_NEWS_API_KEY}\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "            new_links = {article[\"news_url\"] for article in data.get(\"data\", [])}\n",
        "            collected_urls.update(new_links)\n",
        "            print(f\"üì° Page {page}: Collected {len(new_links)} articles. Total: {len(collected_urls)}\")\n",
        "            page += 1\n",
        "            if len(new_links) == 0:  # Stop if no new data is found\n",
        "                break\n",
        "            time.sleep(1)  # Avoid rate limits\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error fetching StockNewsAPI: {e}\")\n",
        "            break\n",
        "    return list(collected_urls)\n",
        "\n",
        "# Function to scrape text from URLs\n",
        "def scrape_text(url):\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.text if is_relevant(article.title) else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Collect news URLs\n",
        "print(f\"\\nüîç Collecting news for {stock_name} ({stock_ticker})...\")\n",
        "urls = set(\n",
        "    get_newsapi_links(500) +\n",
        "    get_google_news_links(500) +\n",
        "    get_yahoo_finance_links(500) +\n",
        "    get_alpha_vantage_links(500) +\n",
        "    get_stocknewsapi_links(5000)\n",
        ")\n",
        "\n",
        "print(f\"üì° Total {stock_name} News URLs Collected: {len(urls)}\")\n",
        "\n",
        "# Collecting sentiment-labeled sentences\n",
        "positive_sentences, negative_sentences, neutral_sentences = [], [], []\n",
        "\n",
        "for url in urls:\n",
        "    text = scrape_text(url)\n",
        "    if text:\n",
        "        extracted_sentences = sent_tokenize(text)\n",
        "        for sentence in extracted_sentences:\n",
        "            if is_relevant(sentence):\n",
        "                sentiment = classify_sentiment(sentence)\n",
        "                if sentiment == \"Positive\" and len(positive_sentences) < 10000:\n",
        "                    positive_sentences.append(sentence)\n",
        "                elif sentiment == \"Negative\" and len(negative_sentences) < 10000:\n",
        "                    negative_sentences.append(sentence)\n",
        "                elif sentiment == \"Neutral\" and len(neutral_sentences) < 10000:\n",
        "                    neutral_sentences.append(sentence)\n",
        "    if len(positive_sentences) >= 10000 and len(negative_sentences) >= 10000 and len(neutral_sentences) >= 10000:\n",
        "        break\n",
        "\n",
        "# Save data to CSV\n",
        "filename = \"amazon_stock_sentiment_data.csv\"\n",
        "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Sentence\", \"Sentiment\"])\n",
        "    writer.writerows([[s, \"Positive\"] for s in positive_sentences])\n",
        "    writer.writerows([[s, \"Negative\"] for s in negative_sentences])\n",
        "    writer.writerows([[s, \"Neutral\"] for s in neutral_sentences])\n",
        "\n",
        "print(f\"‚úÖ {stock_name} news saved to {filename}: {len(positive_sentences)} Positive, {len(negative_sentences)} Negative, {len(neutral_sentences)} Neutral!\\n\")\n",
        "print(\"üéØ Amazon news sentiment analysis completed!\")\n"
      ],
      "metadata": {
        "id": "WPO1Rn3co3us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8530d292-50ed-4627-aa6b-1ed423c40ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Collecting news for Amazon (AMZN)...\n",
            "üì° Page 1: Collected 0 articles. Total: 0\n",
            "üì° Total Amazon News URLs Collected: 178\n",
            "Error fetching https://www.abc.net.au/news/2025-02-26/dozens-quit-elon-musk-s-doge-citing-security-risks-over-us-data/104983062: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.abc.net.au/news/2025-02-26/dozens-quit-elon-musk-s-doge-citing-security-risks-over-us-data/104983062 on URL https://www.abc.net.au/news/2025-02-26/dozens-quit-elon-musk-s-doge-citing-security-risks-over-us-data/104983062\n",
            "Error fetching https://www.forbes.com/sites/tonybradley/2025/02/25/a-holistic-approach-to-modern-threats/: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/tonybradley/2025/02/25/a-holistic-approach-to-modern-threats/ on URL https://www.forbes.com/sites/tonybradley/2025/02/25/a-holistic-approach-to-modern-threats/\n",
            "Error fetching https://www.hindustantimes.com/entertainment/web-series/amazon-prime-video-india-content-head-nikhil-madhok-reveals-why-paatal-lok-2-made-him-nervous-gave-him-sleepless-nights-101740562355724.html&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIARAC&usg=AOvVaw3XUG1S3S49Q42aj_ZGSJSp: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.hindustantimes.com/entertainment/web-series/amazon-prime-video-india-content-head-nikhil-madhok-reveals-why-paatal-lok-2-made-him-nervous-gave-him-sleepless-nights-101740562355724.html&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIARAC&usg=AOvVaw3XUG1S3S49Q42aj_ZGSJSp on URL https://www.hindustantimes.com/entertainment/web-series/amazon-prime-video-india-content-head-nikhil-madhok-reveals-why-paatal-lok-2-made-him-nervous-gave-him-sleepless-nights-101740562355724.html&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIARAC&usg=AOvVaw3XUG1S3S49Q42aj_ZGSJSp\n",
            "Error fetching https://biz.chosun.com/en/en-it/2025/02/27/HGSH5E46G5FGTHCIQVTHN3J3OM/&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIAxAC&usg=AOvVaw2497mmxS3wbox_b6gHq-m-: Article `download()` failed with 404 Client Error: Not Found for url: https://biz.chosun.com/en/en-it/2025/02/27/HGSH5E46G5FGTHCIQVTHN3J3OM/&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIAxAC&usg=AOvVaw2497mmxS3wbox_b6gHq-m-/ on URL https://biz.chosun.com/en/en-it/2025/02/27/HGSH5E46G5FGTHCIQVTHN3J3OM/&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIAxAC&usg=AOvVaw2497mmxS3wbox_b6gHq-m-\n",
            "Error fetching https://www.livelaw.in/high-court/delhi-high-court/amazon-trademark-infringement-luxury-brand-beverly-hills-polo-club-damages-costs-285085&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIBhAC&usg=AOvVaw2aBOus4w2i25mq60j4Evii: Article `download()` failed with 404 Client Error: Not Found for url: https://www.livelaw.in/high-court/delhi-high-court/amazon-trademark-infringement-luxury-brand-beverly-hills-polo-club-damages-costs-285085&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIBhAC&usg=AOvVaw2aBOus4w2i25mq60j4Evii on URL https://www.livelaw.in/high-court/delhi-high-court/amazon-trademark-infringement-luxury-brand-beverly-hills-polo-club-damages-costs-285085&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIBhAC&usg=AOvVaw2aBOus4w2i25mq60j4Evii\n",
            "Error fetching https://www.itp.net/edge/amazon-unveils-alexa-a-smarter-ai-powered-virtual-assistant-for-the-future&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIAhAC&usg=AOvVaw0IdvpbCpSJ7yYzwMRbvmXi: Article `download()` failed with 403 Client Error: HTTP Forbidden for url: https://www.itp.net/edge/amazon-unveils-alexa-a-smarter-ai-powered-virtual-assistant-for-the-future&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIAhAC&usg=AOvVaw0IdvpbCpSJ7yYzwMRbvmXi on URL https://www.itp.net/edge/amazon-unveils-alexa-a-smarter-ai-powered-virtual-assistant-for-the-future&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIAhAC&usg=AOvVaw0IdvpbCpSJ7yYzwMRbvmXi\n",
            "Error fetching https://www.indiatvnews.com/technology/news/amazon-launches-ai-powered-alexa-free-for-prime-members-usd-19-99-for-others-2025-02-27-978179&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIBxAC&usg=AOvVaw0UQ6LmPSk42LE-cYkJCJXa: Article `download()` failed with 404 Client Error: Not Found for url: https://www.indiatvnews.com/technology/news/amazon-launches-ai-powered-alexa-free-for-prime-members-usd-19-99-for-others-2025-02-27-978179&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIBxAC&usg=AOvVaw0UQ6LmPSk42LE-cYkJCJXa on URL https://www.indiatvnews.com/technology/news/amazon-launches-ai-powered-alexa-free-for-prime-members-usd-19-99-for-others-2025-02-27-978179&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIBxAC&usg=AOvVaw0UQ6LmPSk42LE-cYkJCJXa\n",
            "Error fetching https://bleedingcool.com/tv/neagley-star-maria-sten-checks-in-from-reacher-spinoff-series-set/: Article `download()` failed with 403 Client Error: Forbidden for url: https://bleedingcool.com/tv/neagley-star-maria-sten-checks-in-from-reacher-spinoff-series-set/ on URL https://bleedingcool.com/tv/neagley-star-maria-sten-checks-in-from-reacher-spinoff-series-set/\n",
            "Error fetching https://www.commonsensewithmoney.com/learning-resources-sorting-surprise-picnic-baskets-9/: Article `download()` failed with HTTPSConnectionPool(host='www.commonsensewithmoney.com', port=443): Read timed out. (read timeout=7) on URL https://www.commonsensewithmoney.com/learning-resources-sorting-surprise-picnic-baskets-9/\n",
            "Error fetching https://indianexpress.com/article/technology/artificial-intelligence/amazon-introduces-alexa-plus-smarter-ai-assistant-9858827/&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQICRAC&usg=AOvVaw0Sa5HtLSKlj85hM8jRROvr: Article `download()` failed with 404 Client Error: Not Found for url: https://indianexpress.com/article/technology/artificial-intelligence/amazon-introduces-alexa-plus-smarter-ai-assistant-9858827/&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQICRAC&usg=AOvVaw0Sa5HtLSKlj85hM8jRROvr on URL https://indianexpress.com/article/technology/artificial-intelligence/amazon-introduces-alexa-plus-smarter-ai-assistant-9858827/&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQICRAC&usg=AOvVaw0Sa5HtLSKlj85hM8jRROvr\n",
            "Error fetching https://www.businessworld.in/article/amazon-unveils-ai-powered-alexa-in-major-overhaul-549227&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIBBAC&usg=AOvVaw0Xvv7rSpeZBKdScigPV09H: You must `download()` an article first!\n",
            "Error fetching https://screenrant.com/james-bond-amazon-spinoff-young-007-books-tv-show-op-ed/: Article `download()` failed with ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')) on URL https://screenrant.com/james-bond-amazon-spinoff-young-007-books-tv-show-op-ed/\n",
            "Error fetching https://www.latestly.com/socially/technology/alexa-amazon-launches-new-ai-powered-alexa-assistant-with-more-natural-conversation-and-capabilities-free-for-prime-members-and-usd-19-19-for-non-prime-users-6673320.html&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIChAC&usg=AOvVaw0J9J0ti9FPndMIuP--K7wA: Article `download()` failed with 404 Client Error: Not Found for url: https://www.latestly.com/socially/technology/alexa-amazon-launches-new-ai-powered-alexa-assistant-with-more-natural-conversation-and-capabilities-free-for-prime-members-and-usd-19-19-for-non-prime-users-6673320.html&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIChAC&usg=AOvVaw0J9J0ti9FPndMIuP--K7wA on URL https://www.latestly.com/socially/technology/alexa-amazon-launches-new-ai-powered-alexa-assistant-with-more-natural-conversation-and-capabilities-free-for-prime-members-and-usd-19-19-for-non-prime-users-6673320.html&ved=2ahUKEwia9bfNoOOLAxVxRmwGHbS2IEEQxfQBegQIChAC&usg=AOvVaw0J9J0ti9FPndMIuP--K7wA\n",
            "Error fetching https://www.forbes.com/sites/timlammers/2025/02/25/what-time-is-alan-ritchsons-reacher-season-3-episode-4-this-week/: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/timlammers/2025/02/25/what-time-is-alan-ritchsons-reacher-season-3-episode-4-this-week/ on URL https://www.forbes.com/sites/timlammers/2025/02/25/what-time-is-alan-ritchsons-reacher-season-3-episode-4-this-week/\n",
            "‚úÖ Amazon news saved to amazon_stock_sentiment_data.csv: 207 Positive, 29 Negative, 40 Neutral!\n",
            "\n",
            "üéØ Amazon news sentiment analysis completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zk4Lj0z1DQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}